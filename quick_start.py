#!/usr/bin/env python3
"""
å¿«é€Ÿå¯åŠ¨è„šæœ¬
ä¸€é”®è¿è¡Œå®Œæ•´çš„è®­ç»ƒå’Œæ¨ç†æµç¨‹
"""

import os
import sys
import subprocess
import json
import argparse
from pathlib import Path

def run_command(cmd, description, check=True):
    """è¿è¡Œå‘½ä»¤"""
    print(f"\n{'='*60}")
    print(f"ğŸš€ {description}")
    print(f"{'='*60}")
    print(f"Command: {cmd}")
    
    try:
        result = subprocess.run(cmd, shell=True, check=check, capture_output=True, text=True)
        if result.returncode == 0:
            print(f"âœ… {description} completed successfully")
            return True
        else:
            print(f"âŒ {description} failed")
            print(f"Error: {result.stderr}")
            return False
    except subprocess.CalledProcessError as e:
        print(f"âŒ {description} failed: {e}")
        return False

def check_environment():
    """æ£€æŸ¥ç¯å¢ƒ"""
    print("ğŸ” Checking environment...")
    
    # æ£€æŸ¥Pythonç‰ˆæœ¬
    python_version = sys.version_info
    if python_version.major < 3 or (python_version.major == 3 and python_version.minor < 8):
        print("âŒ Python 3.8+ required")
        return False
    print(f"âœ… Python {python_version.major}.{python_version.minor}.{python_version.micro}")
    
    # æ£€æŸ¥CUDA
    try:
        import torch
        if torch.cuda.is_available():
            print(f"âœ… CUDA available: {torch.cuda.get_device_name(0)}")
            print(f"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        else:
            print("âš ï¸  CUDA not available, will use CPU (slower)")
    except ImportError:
        print("âš ï¸  PyTorch not installed yet")
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not os.path.exists("cal_meta.json"):
        print("âŒ cal_meta.json not found")
        return False
    print("âœ… Found cal_meta.json")
    
    if not os.path.exists("cal_data"):
        print("âŒ cal_data directory not found")
        return False
    print("âœ… Found cal_data directory")
    
    return True

def setup_environment():
    """è®¾ç½®ç¯å¢ƒ"""
    print("\nğŸ”§ Setting up environment...")
    
    # å®‰è£…ä¾èµ–
    if not run_command("pip install -r requirements.txt", "Installing dependencies"):
        return False
    
    # è¿è¡Œç¯å¢ƒè®¾ç½®
    if not run_command("python setup_environment.py", "Setting up environment"):
        return False
    
    return True

def train_model(config_file=None):
    """è®­ç»ƒæ¨¡å‹"""
    print("\nğŸ¯ Training model...")
    
    cmd = "python train_lora.py"
    if config_file:
        cmd += f" --config {config_file}"
    
    return run_command(cmd, "Training VLM model with LoRA")

def run_inference(image_path=None, image_dir=None, model_path=None, lora_path=None):
    """è¿è¡Œæ¨ç†"""
    print("\nğŸ”® Running inference...")
    
    if not image_path and not image_dir:
        print("âŒ Please provide either --image_path or --image_dir")
        return False
    
    cmd = "python inference.py"
    
    if model_path:
        cmd += f" --base_model {model_path}"
    else:
        cmd += " --base_model ./models/llava-v1.5-7b"
    
    if lora_path:
        cmd += f" --lora_path {lora_path}"
    else:
        cmd += " --lora_path ./checkpoints/food_vlm_lora/lora_weights"
    
    if image_path:
        cmd += f" --image_path {image_path}"
    elif image_dir:
        cmd += f" --image_dir {image_dir}"
    
    cmd += " --output inference_results.json"
    
    return run_command(cmd, "Running inference")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Food VLM Quick Start")
    parser.add_argument("--mode", choices=["setup", "train", "inference", "full"], 
                       default="full", help="Run mode")
    parser.add_argument("--image_path", type=str, help="Path to single image for inference")
    parser.add_argument("--image_dir", type=str, help="Directory of images for inference")
    parser.add_argument("--model_path", type=str, help="Path to base model")
    parser.add_argument("--lora_path", type=str, help="Path to LoRA weights")
    parser.add_argument("--config", type=str, help="Path to config file")
    parser.add_argument("--skip_setup", action="store_true", help="Skip environment setup")
    
    args = parser.parse_args()
    
    print("ğŸ½ï¸  Food VLM Training Pipeline")
    print("="*60)
    
    # æ£€æŸ¥ç¯å¢ƒ
    if not check_environment():
        print("\nâŒ Environment check failed")
        return False
    
    success = True
    
    # è®¾ç½®ç¯å¢ƒ
    if args.mode in ["setup", "full"] and not args.skip_setup:
        success = setup_environment()
        if not success:
            print("\nâŒ Environment setup failed")
            return False
    
    # è®­ç»ƒæ¨¡å‹
    if args.mode in ["train", "full"] and success:
        success = train_model(args.config)
        if not success:
            print("\nâŒ Training failed")
            return False
    
    # è¿è¡Œæ¨ç†
    if args.mode in ["inference", "full"] and success:
        success = run_inference(
            image_path=args.image_path,
            image_dir=args.image_dir,
            model_path=args.model_path,
            lora_path=args.lora_path
        )
        if not success:
            print("\nâŒ Inference failed")
            return False
    
    if success:
        print("\n" + "="*60)
        print("ğŸ‰ All tasks completed successfully!")
        print("="*60)
        
        if args.mode in ["inference", "full"]:
            print("\nğŸ“Š Check inference_results.json for results")
        
        print("\nğŸ“ Generated files:")
        if os.path.exists("checkpoints"):
            print("  - checkpoints/ (trained model)")
        if os.path.exists("inference_results.json"):
            print("  - inference_results.json (inference results)")
        if os.path.exists("config.json"):
            print("  - config.json (configuration)")
    
    return success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
