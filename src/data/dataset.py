"""
é£Ÿç‰©è¥å…»åˆ†ææ•°æ®é›†
æ”¯æŒå¤šç§VLMæ¨¡å‹çš„æ•°æ®å¤„ç†
"""

import json
import os
import pandas as pd
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader
from typing import Dict, List, Any, Optional
import random
import re
from transformers import DataCollatorForSeq2Seq

class FoodDataset(Dataset):
    """é£Ÿç‰©è¥å…»åˆ†ææ•°æ®é›† - æ”¯æŒå›¾ç‰‡+æ–‡æœ¬å¯¹è¾“å…¥"""
    
    def __init__(
        self, 
        data_path: str, 
        image_dir: str, 
        tokenizer, 
        processor, 
        max_length: int = 512, 
        split: str = "train",
        model_type: str = "llava",
        max_samples: Optional[int] = None
    ):
        self.data_path = data_path
        self.image_dir = image_dir
        self.tokenizer = tokenizer
        self.processor = processor
        self.max_length = max_length
        self.split = split
        self.model_type = model_type
        self.max_samples = max_samples
        
        # åŠ è½½æ•°æ®
        with open(data_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)

        self.filtered_data = self._filter_data()
        
        # åˆ›å»ºè®­ç»ƒæ ·æœ¬
        self.samples = self._create_samples()
        
        # é™åˆ¶æ ·æœ¬æ•°é‡ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
        if self.max_samples and len(self.samples) > self.max_samples:
            self.samples = self.samples[:self.max_samples]
            print(f"Limited to {self.max_samples} samples for quick testing")
        
        print(f"Loaded {len(self.samples)} samples for {split} split using {model_type} model")
    
    def _is_image_valid(self, image_path: str) -> bool:
        """
        æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆï¼ˆæœªæŸåï¼‰
        
        Args:
            image_path: å›¾ç‰‡æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: å›¾ç‰‡æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(image_path):
                return False
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆè‡³å°‘è¦æœ‰ä¸€äº›å†…å®¹ï¼‰
            if os.path.getsize(image_path) < 100:  # è‡³å°‘100å­—èŠ‚
                return False
            
            # å°è¯•æ‰“å¼€å›¾ç‰‡
            with Image.open(image_path) as img:
                # éªŒè¯å›¾ç‰‡æ ¼å¼
                img.verify()
                
                # é‡æ–°æ‰“å¼€å›¾ç‰‡ï¼ˆverify()ä¼šå…³é—­æ–‡ä»¶ï¼‰
                with Image.open(image_path) as img2:
                    # è½¬æ¢ä¸ºRGBæ ¼å¼æµ‹è¯•
                    img2.convert('RGB')
                    
                    # æ£€æŸ¥å›¾ç‰‡å°ºå¯¸
                    width, height = img2.size
                    if width < 10 or height < 10:  # å›¾ç‰‡å¤ªå°
                        return False
                    
                    # æ£€æŸ¥å›¾ç‰‡æ˜¯å¦ä¸ºç©ºï¼ˆå…¨é»‘æˆ–å…¨ç™½ï¼‰
                    # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„æ£€æŸ¥ï¼Œä½†ä¸ºäº†æ€§èƒ½è€ƒè™‘ï¼Œæš‚æ—¶è·³è¿‡
                    
            return True
            
        except Exception as e:
            # è®°å½•æŸåçš„å›¾ç‰‡ï¼ˆå¯é€‰ï¼‰
            # print(f"âš ï¸  Invalid image detected: {image_path} - {str(e)}")
            return False
    
    def _filter_data(self) -> List[Dict]:
        """è¿‡æ»¤æœ‰æ•ˆæ•°æ® - åŒ…å«å›¾ç‰‡å®Œæ•´æ€§æ£€æŸ¥"""
        filtered = []
        corrupted_count = 0
        total_checked = 0
        
        for key, item in self.data.items():
            # æ£€æŸ¥å¿…è¦å­—æ®µ
            if (item.get('image_paths') and 
                item.get('ingredients') and 
                item.get('nutr_per_ingredient') and
                item.get('partition') == self.split):
                
                # æ£€æŸ¥å›¾ç‰‡æ˜¯å¦å­˜åœ¨ä¸”æœªæŸå
                valid_images = []
                for img_path in item['image_paths'][-4:]:  # åªæ£€æŸ¥æœ€å4å¼ å›¾ç‰‡
                    full_path = os.path.join(self.image_dir, img_path)
                    total_checked += 1
                    
                    if self._is_image_valid(full_path):
                        valid_images.append(img_path)
                    else:
                        corrupted_count += 1
                
                if valid_images:
                    item['valid_image_paths'] = valid_images
                    filtered.append((key, item))
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        if total_checked > 0:
            print(f"ğŸ“Š Image validation results for {self.split} split:")
            print(f"   Total images checked: {total_checked}")
            print(f"   Corrupted images: {corrupted_count}")
            print(f"   Valid images: {total_checked - corrupted_count}")
            print(f"   Corruption rate: {corrupted_count/total_checked*100:.1f}%")
        
        return filtered
    
    def _create_samples(self) -> List[Dict]:
        """åˆ›å»ºè®­ç»ƒæ ·æœ¬ - ç»Ÿä¸€QAæ¨¡å¼"""
        samples = []
        
        for key, item in self.filtered_data:
            # éšæœºé€‰æ‹©ä¸€å¼ å›¾ç‰‡
            img_path = random.choice(item['valid_image_paths'])
            full_img_path = os.path.join(self.image_dir, img_path)

            # QAæ¨¡å¼ï¼šåˆ›å»ºå¤šä¸ªé—®ç­”å¯¹
            qa_pairs = self._create_qa_pairs(item)
            for question, answer in qa_pairs:
                samples.append({
                    'image_path': full_img_path,
                    'question': question,
                    'answer': answer,
                    'metadata': {
                        'id': key,
                        'title': item.get('title', ''),
                        'ingredients': item.get('ingredients', []),
                        'nutrition': item.get('nutr_per_ingredient', []),
                        'fsa_lights': item.get('fsa_lights_per100g', {})
                    }
                })
        
        return samples
    
    def _create_qa_pairs(self, item: Dict) -> List[tuple]:
        """åˆ›å»ºé—®ç­”å¯¹"""
        qa_pairs = []
        title = item.get('title', 'Unknown Dish')
        ingredients = [ing['text'] for ing in item.get('ingredients', [])]
        nutrition = item.get('nutr_per_ingredient', [])
        fsa_lights = item.get('fsa_lights_per100g', {})
        quantities = [q['text'] for q in item.get('quantity', [])]
        units = [u['text'] for u in item.get('unit', [])]
        
        # è®¡ç®—æ€»è¥å…»
        total_nutrition = self._calculate_total_nutrition(nutrition)
        
        # è¥å…»ç›¸å…³é—®ç­”
        qa_pairs.extend([
            (
                "How much does this food have, in terms of KCal?",
                f"This food contains approximately {total_nutrition['energy']:.1f} calories (kcal) per 100g."
            ),
            (
                "What is the protein content of this dish?",
                f"This dish contains about {total_nutrition['protein']:.1f}g of protein per 100g."
            ),
            (
                "How much fat is in this food?",
                f"This food contains approximately {total_nutrition['fat']:.1f}g of fat per 100g."
            ),
            (
                "What is the sodium content?",
                f"The sodium content is about {total_nutrition['sodium']:.1f}mg per 100g."
            ),
            (
                "How much sugar does this contain?",
                f"This contains approximately {total_nutrition['sugar']:.1f}g of sugar per 100g."
            )
        ])
        
        # é£Ÿæ+æ•°é‡+å•ä½é—®ç­”
        if ingredients and quantities and units:
            ingredient_with_amounts = []
            for ing, q, u in zip(ingredients, quantities, units):
                ingredient_with_amounts.append(f"{ing} â†’ {q} {u}")
            ingredients_text = "; ".join(ingredient_with_amounts)
            
            qa_pairs.append((
                "What ingredients and quantities are required for this recipe?",
                f"The recipe requires the following ingredients with quantities: {ingredients_text}."
            ))

        # ä»…é£Ÿæç›¸å…³é—®ç­”
        if ingredients:
            ingredients_text = ", ".join(ingredients[:5])  # é™åˆ¶é•¿åº¦
            qa_pairs.extend([
                (
                    "What are the main ingredients in this dish?",
                    f"The main ingredients are: {ingredients_text}."
                ),
                (
                    "What type of ingredients are used?",
                    f"This dish uses various ingredients including {ingredients_text}."
                )
            ])
        
        # å¥åº·æŒ‡æ ‡é—®ç­”
        if fsa_lights:
            qa_pairs.extend([
                (
                    "Is this food healthy in terms of fat content?",
                    f"The fat content is rated as {fsa_lights.get('fat', 'unknown')} (traffic light system)."
                ),
                (
                    "How much salt is in this food?",
                    f"The salt content is rated as {fsa_lights.get('salt', 'unknown')} (traffic light system)."
                ),
                (
                    "What about saturated fat and sugar levels?",
                    f"Saturated fat is rated as {fsa_lights.get('saturates', 'unknown')} and sugar as {fsa_lights.get('sugars', 'unknown')} (traffic light system)."
                )
            ])
        
        # é€šç”¨é—®ç­”
        qa_pairs.extend([
            (
                "What is this dish called?",
                f"This dish is called '{title}'."
            ),
            (
                "Can you describe this food?",
                f"This is '{title}', a dish that appears to be prepared with various ingredients and has specific nutritional characteristics."
            )
        ])
        
        # éšæœºé€‰æ‹©éƒ¨åˆ†é—®ç­”å¯¹ï¼Œé¿å…æ•°æ®è¿‡å¤š
        if len(qa_pairs) > 10:
            qa_pairs = random.sample(qa_pairs, 10)
        
        return qa_pairs

    def _calculate_total_nutrition(self, nutrition: List[Dict]) -> Dict[str, float]:
        """è®¡ç®—æ€»è¥å…»"""
        total_nutrition = {
            'energy': 0, 'protein': 0, 'fat': 0, 
            'saturated_fat': 0, 'sodium': 0, 'sugar': 0
        }
        
        for nutr in nutrition:
            total_nutrition['energy'] += nutr.get('nrg', 0)
            total_nutrition['protein'] += nutr.get('pro', 0)
            total_nutrition['fat'] += nutr.get('fat', 0)
            total_nutrition['saturated_fat'] += nutr.get('sat', 0)
            total_nutrition['sodium'] += nutr.get('sod', 0)
            total_nutrition['sugar'] += nutr.get('sug', 0)
        
        return total_nutrition
    
    def _build_text_description(self, item: Dict) -> str:
        """æ„å»ºæ–‡æœ¬æè¿°"""
        # åŸºç¡€ä¿¡æ¯
        title = item.get('title', 'Unknown Dish')
        ingredients = [ing['text'] for ing in item.get('ingredients', [])]
        nutrition = item.get('nutr_per_ingredient', [])
        fsa_lights = item.get('fsa_lights_per100g', {})
        
        # æ„å»ºé£Ÿææè¿°
        ingredients_text = ", ".join(ingredients[:10])  # é™åˆ¶é•¿åº¦
        
        # æ„å»ºè¥å…»ä¿¡æ¯æè¿°
        nutrition_text = self._format_nutrition_info(nutrition, fsa_lights)
        
        # æ ¹æ®æ¨¡å‹ç±»å‹æ„å»ºä¸åŒçš„æç¤ºè¯
        if self.model_type == "qwen_vl":
            # Qwen-VLæ›´é€‚åˆä¸­æ–‡æç¤º
            description = f"""<image>
èœå“ï¼š{title}

é£Ÿæï¼š{ingredients_text}

è¥å…»æˆåˆ†ï¼ˆæ¯100gï¼‰ï¼š
{nutrition_text}

è¯·åˆ†æè¿™å¼ é£Ÿç‰©å›¾ç‰‡ï¼Œæä¾›ä»¥ä¸‹è¯¦ç»†ä¿¡æ¯ï¼š
1. ä¸»è¦é£ŸæåŠå…¶ç±»å‹
2. è¥å…»æˆåˆ†åŒ…æ‹¬å¡è·¯é‡Œã€è›‹ç™½è´¨ã€è„‚è‚ªã€ç¢³æ°´åŒ–åˆç‰©ã€é’ å’Œç³–
3. å¥åº·æŒ‡æ ‡ï¼ˆè„‚è‚ªã€ç›åˆ†ã€é¥±å’Œè„‚è‚ªã€ç³–åˆ†çš„äº¤é€šç¯é¢œè‰²ï¼‰
4. çƒ¹é¥ªæ–¹æ³•å’Œåˆ¶ä½œé£æ ¼"""
        else:
            # é»˜è®¤è‹±æ–‡æç¤º
            description = f"""<image>
Dish: {title}

Ingredients: {ingredients_text}

Nutritional Information (per 100g):
{nutrition_text}

Please analyze this food image and provide detailed information about:
1. The main ingredients and their types
2. Nutritional content including calories, protein, fat, carbohydrates, sodium, and sugar
3. Health indicators (traffic light colors for fat, salt, saturates, sugars)
4. Cooking method and preparation style"""
        
        return description
    
    def _format_nutrition_info(self, nutrition: List[Dict], fsa_lights: Dict) -> str:
        """æ ¼å¼åŒ–è¥å…»ä¿¡æ¯"""
        if not nutrition:
            return "Nutritional information not available"
        
        # è®¡ç®—æ€»è¥å…»ï¼ˆå‡è®¾æ‰€æœ‰é£Ÿæç­‰é‡ï¼‰
        total_nutrition = {
            'energy': 0, 'protein': 0, 'fat': 0, 
            'saturated_fat': 0, 'sodium': 0, 'sugar': 0
        }
        
        for nutr in nutrition:
            total_nutrition['energy'] += nutr.get('nrg', 0)
            total_nutrition['protein'] += nutr.get('pro', 0)
            total_nutrition['fat'] += nutr.get('fat', 0)
            total_nutrition['saturated_fat'] += nutr.get('sat', 0)
            total_nutrition['sodium'] += nutr.get('sod', 0)
            total_nutrition['sugar'] += nutr.get('sug', 0)
        
        # æ ¼å¼åŒ–è¾“å‡º
        nutrition_text = f"""Energy: {total_nutrition['energy']:.1f} kcal
Protein: {total_nutrition['protein']:.1f}g
Fat: {total_nutrition['fat']:.1f}g
Saturated Fat: {total_nutrition['saturated_fat']:.1f}g
Sodium: {total_nutrition['sodium']:.1f}mg
Sugar: {total_nutrition['sugar']:.1f}g

Traffic Light Colors (per 100g):
- Fat: {fsa_lights.get('fat', 'unknown')}
- Salt: {fsa_lights.get('salt', 'unknown')}
- Saturates: {fsa_lights.get('saturates', 'unknown')}
- Sugars: {fsa_lights.get('sugars', 'unknown')}"""
        
        return nutrition_text
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # åŠ è½½å›¾ç‰‡ - è¿™é‡Œåº”è¯¥ä¸ä¼šå‡ºé”™ï¼Œå› ä¸ºå·²ç»åœ¨è¿‡æ»¤é˜¶æ®µæ£€æŸ¥è¿‡äº†
        try:
            image = Image.open(sample['image_path']).convert('RGB')
        except Exception as e:
            # å¦‚æœä»ç„¶å‡ºé”™ï¼Œä½¿ç”¨é»˜è®¤å›¾ç‰‡
            print(f"âŒ Unexpected error loading image {sample['image_path']}: {e}")
            image = Image.new('RGB', (224, 224), color='white')
        
        # æ„å»ºé—®ç­”æ ¼å¼ï¼ˆç»Ÿä¸€QAæ¨¡å¼ï¼‰
        question = sample['question']
        answer = sample['answer']
        
        if self.model_type == "qwen_vl":
            conversation = f"<image>\nHuman: {question}\nAssistant: {answer}"
        else:
            # ä½¿ç”¨LLaVAæ ‡å‡†æ ¼å¼
            conversation = f"<image>\nUSER: {question}\nASSISTANT: {answer}"
        
        # å¤„ç†å›¾ç‰‡å’Œæ–‡æœ¬
        inputs = self.processor(
            text=conversation,
            images=image,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=self.max_length
        )
        
        for key in inputs:
            if isinstance(inputs[key], torch.Tensor):
                inputs[key] = inputs[key].squeeze(0)
        
        # æ­£ç¡®è®¾ç½®æ ‡ç­¾ - åªå¯¹å›ç­”éƒ¨åˆ†è®¡ç®—æŸå¤±
        labels = self._create_labels(inputs['input_ids'], conversation, question)
        
        return {
            'input_ids': inputs['input_ids'],
            'attention_mask': inputs['attention_mask'],
            'pixel_values': inputs['pixel_values'],
            'labels': labels
        }
    
    def _create_labels(self, input_ids, conversation, question):
        """åˆ›å»ºæ­£ç¡®çš„æ ‡ç­¾ï¼Œåªå¯¹å›ç­”éƒ¨åˆ†è®¡ç®—æŸå¤±"""
        labels = input_ids.clone()
        
        # æ‰¾åˆ°é—®é¢˜ç»“æŸå’Œå›ç­”å¼€å§‹çš„ä½ç½®
        conversation_tokens = self.tokenizer.encode(conversation, add_special_tokens=False)
        question_tokens = self.tokenizer.encode(f"<image>\nUSER: {question}\nASSISTANT: ", add_special_tokens=False)
        
        # å°†é—®é¢˜éƒ¨åˆ†å’Œç‰¹æ®Štokenè®¾ä¸º-100ï¼ˆä¸è®¡ç®—æŸå¤±ï¼‰
        if len(question_tokens) < len(input_ids):
            labels[:len(question_tokens)] = -100
        
        # ç¡®ä¿pad tokenä¹Ÿè¢«æ©ç 
        pad_token_id = self.tokenizer.pad_token_id
        if pad_token_id is not None:
            labels[labels == pad_token_id] = -100
            
        return labels


class FoodDataLoader:
    """é£Ÿç‰©æ•°æ®åŠ è½½å™¨å·¥å‚"""
    
    @staticmethod
    def create_data_loaders(
        data_path: str, 
        image_dir: str, 
        tokenizer, 
        processor, 
        batch_size: int = 4, 
        max_length: int = 512, 
        num_workers: int = 4,
        model_type: str = "llava",
        max_samples: Optional[int] = None,
    ) -> tuple:
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""

        print(f"Data path: {data_path}")
        print(f"Image dir: {image_dir}")
        
        # åˆ›å»ºè®­ç»ƒé›†
        train_dataset = FoodDataset(
            data_path=data_path,
            image_dir=image_dir,
            tokenizer=tokenizer,
            processor=processor,
            max_length=max_length,
            split="train",
            model_type=model_type,
            max_samples=max_samples,
        )
        
        # åˆ›å»ºéªŒè¯é›†
        val_dataset = FoodDataset(
            data_path=data_path,
            image_dir=image_dir,
            tokenizer=tokenizer,
            processor=processor,
            max_length=max_length,
            split="val",
            model_type=model_type,
            max_samples=max_samples // 10 if max_samples else None,  # éªŒè¯é›†ä½¿ç”¨10%çš„æ ·æœ¬
        )
        
        # åˆ›å»ºæµ‹è¯•é›†
        test_dataset = FoodDataset(
            data_path=data_path,
            image_dir=image_dir,
            tokenizer=tokenizer,
            processor=processor,
            max_length=max_length,
            split="test",
            model_type=model_type,
            max_samples=max_samples // 10 if max_samples else None,  # æµ‹è¯•é›†ä½¿ç”¨10%çš„æ ·æœ¬
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            collate_fn=FoodDataLoader.collate_fn
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            collate_fn=FoodDataLoader.collate_fn
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            collate_fn=FoodDataLoader.collate_fn
        )
        
        return train_loader, val_loader, test_loader
    
    @staticmethod
    def collate_fn(batch):
        """è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•° - ä¿®å¤ç‰ˆæœ¬ï¼Œæ”¯æŒåŠ¨æ€padding"""
        # è·å–æ‰¹æ¬¡ä¸­æ‰€æœ‰å¼ é‡çš„æœ€å¤§é•¿åº¦
        max_input_length = max(item['input_ids'].size(0) for item in batch)
        max_attention_length = max(item['attention_mask'].size(0) for item in batch)
        max_labels_length = max(item['labels'].size(0) for item in batch)
        
        # ç¡®ä¿æ‰€æœ‰é•¿åº¦ä¸€è‡´ï¼ˆåº”è¯¥éƒ½ç›¸åŒï¼Œä½†ä¸ºäº†å®‰å…¨èµ·è§ï¼‰
        max_length = max(max_input_length, max_attention_length, max_labels_length)
        
        # åŠ¨æ€paddingæ‰€æœ‰å¼ é‡åˆ°ç›¸åŒé•¿åº¦
        padded_input_ids = []
        padded_attention_masks = []
        padded_pixel_values = []
        padded_labels = []
        
        for item in batch:
            # Padding input_ids
            input_ids = item['input_ids']
            if input_ids.size(0) < max_length:
                pad_length = max_length - input_ids.size(0)
                # ä½¿ç”¨pad_token_idè¿›è¡Œpaddingï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨0
                pad_token_id = 0  # é€šå¸¸0æ˜¯pad token
                padding = torch.full((pad_length,), pad_token_id, dtype=input_ids.dtype)
                input_ids = torch.cat([input_ids, padding])
            padded_input_ids.append(input_ids)
            
            # Padding attention_mask
            attention_mask = item['attention_mask']
            if attention_mask.size(0) < max_length:
                pad_length = max_length - attention_mask.size(0)
                padding = torch.zeros(pad_length, dtype=attention_mask.dtype)
                attention_mask = torch.cat([attention_mask, padding])
            padded_attention_masks.append(attention_mask)
            
            # Pixel values ä¸éœ€è¦paddingï¼Œå› ä¸ºå›¾ç‰‡å°ºå¯¸æ˜¯å›ºå®šçš„
            padded_pixel_values.append(item['pixel_values'])
            
            # Padding labels
            labels = item['labels']
            if labels.size(0) < max_length:
                pad_length = max_length - labels.size(0)
                # æ ‡ç­¾çš„paddingä½¿ç”¨-100ï¼ˆå¿½ç•¥æŸå¤±ï¼‰
                padding = torch.full((pad_length,), -100, dtype=labels.dtype)
                labels = torch.cat([labels, padding])
            padded_labels.append(labels)
        
        # å †å æ‰€æœ‰å¼ é‡
        input_ids = torch.stack(padded_input_ids)
        attention_mask = torch.stack(padded_attention_masks)
        pixel_values = torch.stack(padded_pixel_values)
        labels = torch.stack(padded_labels)
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'pixel_values': pixel_values,
            'labels': labels
        }

def main():
    """æµ‹è¯•æ•°æ®åŠ è½½å™¨"""
    from transformers import AutoTokenizer, AutoProcessor
    
    # æµ‹è¯•ä¸åŒæ¨¡å‹çš„æ•°æ®åŠ è½½
    models_to_test = [
        ("llava-hf/llava-1.5-7b-hf", "llava"),
        # ("Qwen/Qwen-VL-7B", "qwen_vl")
    ]
    
    for model_id, model_type in models_to_test:
        print(f"\nTesting {model_type} model...")
        
        try:
            # åŠ è½½tokenizerå’Œprocessor
            tokenizer = AutoTokenizer.from_pretrained(model_id)
            processor = AutoProcessor.from_pretrained(model_id)
            
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            train_loader, val_loader, test_loader = FoodDataLoader.create_data_loaders(
                data_path="cal_meta_split.json",
                image_dir="/root/autodl-tmp/data/cal_data",
                tokenizer=tokenizer,
                processor=processor,
                batch_size=2,
                model_type=model_type
            )
            
            # æµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡
            for batch in train_loader:
                print(f"Batch shape for {model_type}:")
                print(f"Input IDs: {batch['input_ids'].shape}")
                print(f"Attention Mask: {batch['attention_mask'].shape}")
                print(f"Pixel Values: {batch['pixel_values'].shape}")
                print(f"Metadata: {len(batch['metadata'])} items")
                break
                
        except Exception as e:
            print(f"Failed to test {model_type}: {e}")

if __name__ == "__main__":
    main()
